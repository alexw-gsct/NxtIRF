#' Searches a specified path for files of a particular file pattern
#'
#' This convenience function identifies files with the specified suffix.
#' The sample name is assumed to be the base name of the file 
#' `use_subdir = FALSE`, but can also be changed to extract the names 
#' of the parent directory `use_subdir = TRUE`. See example below.
#'
#' @param sample_path: The path in which to recursively search for files
#'   that match the given `suffix`
#' @param suffix: A string that specifies the file suffix (e.g. '.bam'
#'   denotes BAM files).
#' @param use_subdir: Whether to assume the directory name containing
#'   the found files denote the sample name. If `FALSE`, the base name
#'   of the file is assumed to be the sample name. See below example.
#' @return A 2-column data frame with the first column containing
#'   the sample name, and the second column being the file path.
#' @examples
#' ## If your sample files are contained like this:
#' ## /path/to/project/samples/Apple.BAM and /path/to/project/samples/Banana.BAM
#' ## df = FindSamples(sample_path = '/path/to/project/samples', suffix = ".BAM",
#'   use_subdir = FALSE)
#' ## returns df with df$sample = c("Apple", "Banana"), 
#' ## df$path = c("/path/to/project/samples/Apple.BAM",
#' ## "/path/to/project/samples/Banana.BAM")
#'
#' ## Alternately, if your sample files are contained like this:
#' ## /path/to/project/samples/Apple/Aligned.BAM and
#' ## /path/to/project/samples/Banana/Aligned.BAM
#' ## df = FindSamples(sample_path = '/path/to/project/samples', suffix = ".BAM",
#' ##  use_subdir = TRUE)
#' ## returns df with df$sample = c("Apple", "Banana"), 
#' ## df$path = c("/path/to/project/samples/Apple/Aligned.BAM",
#' ## "/path/to/project/samples/Banana/Aligned.BAM")
#' @md
#' @export
FindSamples <- function(sample_path, suffix = ".txt.gz", use_subdir = FALSE) {
    assert_that(dir.exists(sample_path),
        msg = "Given path does not exist")
    
    files_found = list.files(pattern = paste0("\\", suffix, "$"),
        path = normalizePath(sample_path), full.names = TRUE, recursive = TRUE)
    if(length(files_found) > 0) {
      df = data.frame(sample = "", path = files_found)
      if(use_subdir) {
          df$sample = basename(dirname(df$path))
      } else {
          df$sample = sub(suffix,"",basename(df$path))
      }
      return(df)
    } else {
      return(NULL)
    }
}

#' Processes data from IRFinder output
#'
#' CollateData unifies a list of IRFinder output files belonging to an experiment.
#' It is assumed every sample is analysed using the same IRFinder reference.
#' The combination of junction counts and IR quantification from IRFinder is used
#' to calculate percentage spliced in (PSI) of alternative splice events, and percent
#' intron retention (PIR) of retained introns. Also, QC information is extracted,
#' and data is collated into fst files for fast downstream access such as `MakeSE()`
#'
#' @param Experiment: A 2-column data frame (generated by `FindSamples()), with
#'   the first column designating the sample names, and the 2nd column containing
#'   the primary IRFinder output file (of type `.txt.gz`)
#' @param reference_path: THe path to the reference generated by BuildReference()
#' @param output_path: The path for the output files to be generated by this function.
#' @param IRMode: The algorithm to calculate 'splice abundance' in IR quantification.
#'   The original algorithm by Middleton et al (2017) proposes `SpliceMax`, which
#'   calculates the number of mapped splice events that share the boundary coordinate
#'   of either the left or right flanking exon (SpliceLeft, SpliceRight) and defines 
#'   splice abundance as the larger of the two values. NxtIRF proposes a new algorithm,
#'   `SpliceOverMax`, to account for the possibility that the major isoform shares neither 
#'   boundary, but arises from either of the flanking "exon islands". Exon islands are 
#'   contiguous regions covered by exons from any transcript (except those designated
#'   as 'retained_intron' or 'sense_intronic'), and are separated by obligate
#'   intronic regions (genomic regions that are introns for all transcripts). Note for
#'   introns that are internal to a single exon island (i.e. akin to "known-exon"
#'   introns from IRFinder), `SpliceOverMax` uses `findOverlaps()` to summate competing
#'   mapped splice reads.
#' @param localHub: See `?AnnotationHub::AnnotationHub()`. Setting `TRUE` will run `AnnotationHub()`
#'   in offline mode
#' @param ah: An AnnotationHub object containing the records `ah_genome` and/or `ah_transcriptome`
#'   records to be used.
#' @param low_memory_mode: Use this mode in memory-limited systems with many samples (> 16).
#'   CollateData will write to file for every N samples as defined by `samples_per_block = N`.
#' @param samples_per_block: How many samples to process per thread. Use in conjunction
#'   with low_memory_mode to lower memory requirements
#' @param n_threads: The number of threads to use.
#'   records to be used.
#' @export
CollateData <- function(Experiment, reference_path, output_path, 
        IRMode = c("SpliceOverMax", "SpliceMax"), 
        localHub = FALSE, ah = AnnotationHub(localHub = localHub), 
        low_memory_mode = FALSE, samples_per_block = 16, n_threads = 1) {
################################################################################
        
    assert_that(is(Experiment, "data.frame"),
        msg = "Experiment object needs to be a data frame")
    assert_that(ncol(Experiment) >= 2,
        msg = paste("Experiment needs to contain at least two columns,",
        "with the first 2 columns containing",
        "(1) sample name and (2) IRFinder output"))
    assert_that(file.exists(file.path(reference_path, "settings.Rds")),
        msg = paste(file.path(reference_path, "settings.Rds"),
            "does not exist"))

    IRMode = match.arg(IRMode)
    assert_that(IRMode != "",
      msg = "IRMode must be either 'SpliceOverMax' (default) or 'SpliceMax'")

    BPPARAM = BiocParallel::bpparam()
    n_threads_to_use = as.numeric(n_threads)
    assert_that(!is.na(n_threads_to_use),
        msg = "n_threads must be a numeric value")
    assert_that(n_threads_to_use <= (parallel::detectCores() - 2) | n_threads_to_use == 1,
      msg = paste("NxtIRF does not support more threads than",
        "parallel::detectCores() - 2") )
    
    if(Sys.info()["sysname"] == "Windows") {
      BPPARAM_mod = BiocParallel::SnowParam(n_threads_to_use)
      message(paste("Using SnowParam", BPPARAM_mod$workers, "threads"))
    } else {
      BPPARAM_mod = BiocParallel::MulticoreParam(n_threads_to_use)
      message(paste("Using MulticoreParam", BPPARAM_mod$workers, "threads"))
    }

    settings = readRDS(file.path(reference_path, "settings.Rds"))
    if(settings$ah_genome != "") {
        genome = FetchAH(settings$ah_genome, localHub = localHub)
    } else {
        genome = rtracklayer::TwoBitFile(file.path(reference_path, 
            "resource", "genome.2bit"))
    }
    
    colnames(Experiment)[1:2] = c("sample", "path")
    assert_that(all(vapply(Experiment$path, file.exists, logical(1))),
        msg = "Some files in Experiment do not exist")
    assert_that(dir.exists(dirname(output_path)),
        msg = paste("Parent directory of output path:", 
        dirname(output_path), "needs to exist"))
        
    # Create a subdirectory for each sample within output_path
    base_output_path = normalizePath(dirname(output_path)) 
    norm_output_path = file.path(base_output_path, basename(output_path))
    if(!dir.exists(norm_output_path)) {
        dir.create(norm_output_path)
    }
    temp_output_path = file.path(norm_output_path, "temp")
    if(!dir.exists(temp_output_path)) {
        dir.create(temp_output_path)
    }		
    if(!dir.exists(file.path(norm_output_path, "samples"))) {
        dir.create(file.path(norm_output_path, "samples"))
    }

    df.internal = as.data.table(Experiment[,1:2])
    df.internal$paired = FALSE
    df.internal$strand = 0
    df.internal$depth = 0
    df.internal$mean_frag_size = 0
    df.internal$directionality_strength = 0
    df.internal$Intergenic_Fraction = 0
    df.internal$rRNA_Fraction = 0
    df.internal$NonPolyA_Fraction = 0
    df.internal$Mitochondrial_Fraction = 0
    df.internal$Unanno_Jn_Fraction = 0
    df.internal$Fraction_Splice_Reads = 0
    df.internal$Fraction_Span_Reads = 0

    df.internal$IRBurden_clean = 0
    df.internal$IRBurden_exitrons = 0
    df.internal$IRBurden_antisense = 0
    
    n_jobs = min(ceiling(nrow(df.internal) / samples_per_block), BPPARAM_mod$workers)
    jobs = NxtIRF.SplitVector(seq_len(nrow(df.internal)), n_jobs)	
	n_jobs = length(jobs)

    
################################################################################
    if(!is.null(shiny::getDefaultReactiveDomain())) {
      shiny::incProgress(0.01, message = "Compiling Sample Stats")
    }
    message("Compiling Sample Stats")
    df.internal = suppressWarnings(rbindlist(
        BiocParallel::bplapply(
            seq_len(n_jobs),
            function(x, jobs, df.internal) {
                suppressPackageStartupMessages({
                    requireNamespace("data.table")
                    requireNamespace("stats")
                })
                work = jobs[[x]]
                block = df.internal[work]
                for(i in seq_len(length(work))) {
                    data.list = get_multi_DT_from_gz(
                        normalizePath(block$path[i]), 
                        c("BAM", "Directionality", "QC")) 
                        # "ROIname", "ChrCoverage", "JC_seqname"))

                    stats = data.list$BAM
                    direct = data.list$Directionality
                    QC = data.list$QC
                    # ROI = data.list$ROIname
                    # ChrCov = data.list$ChrCoverage
                    # junc = data.list$JC_seqname

                    if(stats$Value[3] == 0 & stats$Value[4] > 0) {
                        block$paired[i] = TRUE
                        block$depth[i] = stats$Value[4]
                        block$mean_frag_size[i] = stats$Value[2] / 
                            stats$Value[4]
                    } else if(stats$Value[3] > 0 && 
                            stats$Value[4] / stats$Value[3] / 1000) {
                        block$paired[i] = TRUE
                        block$depth[i] = stats$Value[4]
                        block$mean_frag_size[i] = stats$Value[2] / 
                            stats$Value[4]
                    } else {
                        block$paired[i] = FALSE
                        block$depth[i] = stats$Value[3]
                        block$mean_frag_size[i] = stats$Value[2] / 
                            stats$Value[3]
                    }
                    block$strand[i] = direct$Value[9]

                    # QC
                    block$directionality_strength[i] = direct$Value[8]
                    block$Intergenic_Fraction[i] =
                        QC$Value[QC$QC == "Intergenic Reads"] / 
                            block$depth[i]
                    block$rRNA_Fraction[i] =    
                        QC$Value[QC$QC == "rRNA Reads"] / 
                            block$depth[i]
                    block$NonPolyA_Fraction[i] =
                        QC$Value[QC$QC == "NonPolyA Reads"] / 
                            block$depth[i]
                    block$Mitochondrial_Fraction[i] =
                        QC$Value[QC$QC == "Mitochondrial Reads"] / 
                            block$depth[i]
                    block$Unanno_Jn_Fraction[i] =
                        QC$Value[QC$QC == "Unannotated Junctions"] / 
                        (QC$Value[QC$QC == "Unannotated Junctions"] +
                        QC$Value[QC$QC == "Annotated Junctions"])
                    block$Fraction_Splice_Reads[i] =
                        QC$Value[QC$QC == "Annotated Junctions"] / 
                        block$depth[i]
                    block$Fraction_Span_Reads[i] =
                        QC$Value[QC$QC == "Spans Reads"] / 
                            block$depth[i]

# IRBurden calculations                            
                    if(block$strand[i] == 0) {
                        block$IRBurden_clean[i] =
                            QC$Value[QC$QC == "Non-Directional Clean IntronDepth Sum"] / 
                                QC$Value[QC$QC == "Annotated Junctions"]
                        block$IRBurden_exitrons[i] =
                            QC$Value[QC$QC == "Non-Directional Known-Exon IntronDepth Sum"] / 
                                QC$Value[QC$QC == "Annotated Junctions"]
                        block$IRBurden_antisense[i] =
                            QC$Value[QC$QC == "Non-Directional Anti-Sense IntronDepth Sum"] / 
                                QC$Value[QC$QC == "Annotated Junctions"]
                    } else {
                        block$IRBurden_clean[i] =
                            QC$Value[QC$QC == "Directional Clean IntronDepth Sum"] / 
                                QC$Value[QC$QC == "Annotated Junctions"]
                        block$IRBurden_exitrons[i] =
                            QC$Value[QC$QC == "Directional Known-Exon IntronDepth Sum"] / 
                                QC$Value[QC$QC == "Annotated Junctions"]
                        block$IRBurden_antisense[i] = 0
                    }
                    
                }
                return(block)
            }, jobs = jobs, df.internal = df.internal, BPPARAM = BPPARAM_mod
        )
    ))
    
    if(any(df.internal$strand == 0)) {
        runStranded = FALSE
    } else {
        runStranded = TRUE
    }
    
    # TODO: Simply check version of reference used to generate the IRFinder files
    
    
    # Compile junctions and IR lists first, save to temp files
    if(!is.null(shiny::getDefaultReactiveDomain())) {
      shiny::incProgress(0.15, message = "Compiling Junction List")
    }
    message("Compiling Junction List")       
    # Compile junc.common via merge
    junc.list = suppressWarnings(BiocParallel::bplapply(
        seq_len(n_jobs),
      function(x, jobs, df.internal, temp_output_path) {
        suppressPackageStartupMessages({
          requireNamespace("data.table")
          requireNamespace("stats")
        })
        work = jobs[[x]]
        block = df.internal[work]
        junc.segment = NULL
        for(i in seq_len(length(work))) {
          # junc = get_multi_DT_from_gz(block$path[i], c("JC_seqname"))
          # junc = junc$JC_seqname
          junc = suppressWarnings(data.table::as.data.table(
            data.table::fread(block$path[i], skip = "JC_seqname")))
          data.table::setnames(junc, "JC_seqname", "seqnames")
          if(is.null(junc.segment)) {
              junc.segment = junc[,1:4]
          } else {
              junc.segment = merge(junc.segment, junc[,1:4], all = TRUE)
          }
        # Write temp file
            fst::write.fst(as.data.frame(junc), 
                file.path(temp_output_path, paste(block$sample[i], "junc.fst.tmp", sep=".")))        
        }
        return(junc.segment)
      }, jobs = jobs, df.internal = df.internal, temp_output_path = temp_output_path, BPPARAM = BPPARAM_mod
    ))
    junc.common = NULL
    for(i in seq_len(length(junc.list))) {
      if(is.null(junc.common)) {
        junc.common = junc.list[[i]]
      } else {
        junc.common = merge(junc.common, junc.list[[i]], all = TRUE, by = colnames(junc.common))
      }
    }
    rm(junc.list)
    gc()
    if(!is.null(shiny::getDefaultReactiveDomain())) {
      shiny::incProgress(0.15, message = "Compiling Intron Retention List")
    }
    
    message("Compiling Intron Retention List")
  if(!runStranded) {
    irf = suppressWarnings(as.data.table(
        fread(df.internal$path[i], skip = "Nondir_")))
    setnames(irf, c("Nondir_Chr", "Start", "End", "Strand"), c("seqnames","start","end", "strand"))
  } else {
    irf = suppressWarnings(as.data.table(
        fread(df.internal$path[i], skip = "Dir_Chr")))
    setnames(irf, c("Dir_Chr", "Start", "End", "Strand"), c("seqnames","start","end", "strand"))          
  }
  irf.common = irf[,1:6]
  rm(irf)
    irf.list = suppressWarnings(BiocParallel::bplapply(
        seq_len(n_jobs),
      function(x, jobs, df.internal, temp_output_path, runStranded) {
        suppressPackageStartupMessages({
          requireNamespace("data.table")
          requireNamespace("stats")
          requireNamespace("openssl")
        })
        work = jobs[[x]]
        block = df.internal[work]
        irf.md5 = NULL
        for(i in seq_len(length(work))) {
        
          if(!runStranded) {
            irf = suppressWarnings(data.table::as.data.table(
                data.table::fread(block$path[i], skip = "Nondir_")))
            data.table::setnames(irf, c("Nondir_Chr", "Start", "End", "Strand"), c("seqnames","start","end", "strand"))
          } else {
            irf = suppressWarnings(data.table::as.data.table(
                data.table::fread(block$path[i], skip = "Dir_Chr")))
            data.table::setnames(irf, c("Dir_Chr", "Start", "End", "Strand"), c("seqnames","start","end", "strand"))          
          }
          if(is.null(irf.md5)) {
              irf.md5 = openssl::md5(paste(irf$Name, collapse=" "))
          } else {
              irf.md5 = unique(c(irf.md5, openssl::md5(paste(irf$Name, collapse=" "))))
          }
          fst::write.fst(as.data.frame(irf), 
            file.path(temp_output_path, paste(block$sample[i], "irf.fst.tmp", sep=".")))
        }
        return(irf.md5)
      }, jobs = jobs, df.internal = df.internal, temp_output_path = temp_output_path, 
        runStranded = runStranded, BPPARAM = BPPARAM_mod
    ))
    irf.md5.check = NULL
    for(i in seq_len(length(irf.list))) {
      if(is.null(irf.md5.check)) {
        irf.md5.check = irf.list[[i]]
      } else {
        irf.md5.check = unique(irf.md5.check, irf.list[[i]])
      }
    }
    rm(irf.list)
    gc()

    assert_that(length(irf.md5.check) == 1,
        msg = paste(
            "MD5 check of IRFinder introns are not the same.",
            "Perhaps some samples were processed by a different reference"
        )
    )

    irf.common[, start := start + 1]
    junc.common[, start := start + 1]

# Reassign +/- based on junctions.fst annotation
    # Annotate junctions
  motif_pos <- motif_infer_strand <- Event <- transcript_biotype_2 <- transcript_biotype <- 
	transcript_support_level <- JG_up <- JG_down <- gene_group_left <- exon_group_left <- gene_group_right <- 
	exon_group_right <- IRG_up <- IRG_down <- EventRegion <- Event1a <- Event2a <- up_1a <- 
	i.gene_group_stranded <- i.exon_group_stranded_upstream <- down_1a <- i.exon_group_stranded_downstream <- 
	down_2a <- EventType <- EventName <- i.EventName <- i.transcript_biotype <- NULL
	
	
    if(!is.null(shiny::getDefaultReactiveDomain())) {
      shiny::incProgress(0.15, message = "Tidying up splice junctions and intron retentions")
    }  
    message("Tidying up splice junctions and intron retentions")
    
    candidate.introns = as.data.table(read.fst(file.path(reference_path, "fst", "junctions.fst")))

    junc.strand = unique(candidate.introns[, c("seqnames", "start", "end", "strand")])

    junc.common[, strand := NULL]
    junc.common = unique(junc.common)
    junc.common = merge(junc.common, junc.strand, all = TRUE, by = c("seqnames", "start", "end"))
    junc.common[is.na(strand), strand := "*"]

    left.gr = GRanges(seqnames = junc.common$seqnames, 
        ranges = IRanges(start = junc.common$start, end = junc.common$start + 1), strand = "+")
    right.gr = GRanges(seqnames = junc.common$seqnames, 
        ranges = IRanges(start = junc.common$end - 1, end = junc.common$end), strand = "+")
        
    left.seq = getSeq(genome, left.gr)
    right.seq = getSeq(genome, right.gr)

    junc.common$motif_pos = paste0(as.character(left.seq), as.character(right.seq))
    junc.common$motif_infer_strand = "n"
    junc.common[ motif_pos %in% c("GTAG", "GCAG", "ATAC", "ATAG"), motif_infer_strand := "+"]
    junc.common[ motif_pos %in% c("CTAC", "CTGC", "GTAT", "CTAT"), motif_infer_strand := "-"]
    junc.common[ motif_pos %in% c("GTAC"), motif_infer_strand := "n"]       # Do not accept un-annotated GTACs - too confusing
    # Exclude non-splice motifs (that are also not annotated - i.e. strand == "*")
    junc.common = junc.common[motif_infer_strand != "n" | strand != "*"]

    # Use motif_infer_strand (only for *)
    junc.common[strand == "*", strand := motif_infer_strand]
    junc.common$motif_infer_strand = NULL
    
    # Should splicing across gene groups be allowed? Exclude
    Genes = makeGRangesFromDataFrame(
        read.fst(file.path(reference_path, "fst", "Genes.fst"))
    )

    # Exclude distant splice events:

    # Genes.Group.stranded = as.data.table(
        # reduce(c(Genes, flank(Genes, 5000),
        # flank(Genes, 5000, start = FALSE))
    # ))
    # setorder(Genes.Group.stranded, seqnames, start, strand)
    # Genes.Group.stranded[, gene_group_stranded := .I]
    
    # junc.common.left = copy(junc.common)
    # junc.common.left[, start := start - 1]
    # junc.common.left[, end := start + 1]
    # OL = suppressWarnings(
        # findOverlaps(
            # makeGRangesFromDataFrame(as.data.frame(junc.common.left)), 
            # makeGRangesFromDataFrame(as.data.frame(Genes.Group.stranded))
        # )
    # )
    # junc.common$gene_group_left[OL@from] = Genes.Group.stranded$gene_group_stranded[OL@to]

    # junc.common.right = copy(junc.common)
    # junc.common.right[, end := end + 1]
    # junc.common.right[, start := end - 1]
    # OL = suppressWarnings(
        # findOverlaps(
            # makeGRangesFromDataFrame(as.data.frame(junc.common.right)), 
            # makeGRangesFromDataFrame(as.data.frame(Genes.Group.stranded))
        # )
    # )
    # junc.common$gene_group_right[OL@from] = Genes.Group.stranded$gene_group_stranded[OL@to]
        

    # junc.common = junc.common[gene_group_left == gene_group_right & !is.na(gene_group_left)]
    # junc.common$gene_group_left = NULL
    # junc.common$gene_group_right = NULL
    
    # Assign region names to junctions:
    junc.common[, Event := paste0(seqnames, ":", start, "-", end, "/", strand)]
    
    candidate.introns[, transcript_biotype_2 := transcript_biotype]
    candidate.introns[!(transcript_biotype %in% c("protein_coding", "processed_transcript",
        "lincRNA", "antisense", "nonsense_mediated_decay")), transcript_biotype_2 := "other"]

    candidate.introns[, transcript_biotype_2 := factor(transcript_biotype_2, c("protein_coding", "processed_transcript",
        "lincRNA", "antisense", "other", "nonsense_mediated_decay"), ordered = TRUE)]
        
    if("transcript_support_level" %in% colnames(candidate.introns)) {
        setorder(candidate.introns, transcript_biotype_2, transcript_support_level)
    } else {
        setorder(candidate.introns, transcript_biotype_2)    
    }
    introns.unique = unique(candidate.introns, by = c("seqnames", "start", "end", "width", "strand"))
    setorder(introns.unique, seqnames, start, end, strand)

    junc.annotation = introns.unique[junc.common, 
        c("seqnames", "start", "end", "strand", "transcript_id", "intron_number", "gene_name", "gene_id", "transcript_biotype"),
        on = c("seqnames", "start", "end", "strand")]
    
    # Use Exon Groups file to designate exon groups to all junctions
    Exon.Groups = makeGRangesFromDataFrame(
        read.fst(file.path(reference_path, "fst", "Exons.Group.fst")),
        keep.extra.columns = TRUE)
    
    # Always calculate stranded for junctions
    # if(!runStranded) {
        # Exon.Groups = Exon.Groups[strand(Exon.Groups) == "*"]
    # } else {
        # Exon.Groups = Exon.Groups[strand(Exon.Groups) != "*"]    
    # }
    Exon.Groups.S = Exon.Groups[strand(Exon.Groups) != "*"]    
    
    junc.common.left = copy(junc.common)
    junc.common.left[, start := start - 1]
    junc.common.left[, end := start + 1]
    OL = suppressWarnings(
        findOverlaps(
            makeGRangesFromDataFrame(as.data.frame(junc.common.left)), 
            makeGRangesFromDataFrame(as.data.frame(Exon.Groups.S))
        )
    )
    junc.common[, gene_group_left := NA]
    junc.common[, exon_group_left := NA]
    junc.common$gene_group_left[OL@from] = Exon.Groups.S$gene_group[OL@to]
    junc.common$exon_group_left[OL@from] = Exon.Groups.S$exon_group[OL@to]

    junc.common.right = copy(junc.common)
    junc.common.right[, end := end + 1]
    junc.common.right[, start := end - 1]
    OL = suppressWarnings(
        findOverlaps(
            makeGRangesFromDataFrame(as.data.frame(junc.common.right)), 
            makeGRangesFromDataFrame(as.data.frame(Exon.Groups.S))
        )
    )
    junc.common[, gene_group_right := NA]
    junc.common[, exon_group_right := NA]
    junc.common$gene_group_right[OL@from] = Exon.Groups.S$gene_group[OL@to]
    junc.common$exon_group_right[OL@from] = Exon.Groups.S$exon_group[OL@to]
    
    junc.common[, JG_up := ""]
    junc.common[, JG_down := ""]
    junc.common[strand == "+" & !is.na(gene_group_left) & !is.na(exon_group_left), 
        JG_up := paste(gene_group_left, exon_group_left, sep="_")]
    junc.common[strand == "-" & !is.na(gene_group_right) & !is.na(exon_group_right), 
        JG_up := paste(gene_group_right, exon_group_right, sep="_")]
    junc.common[strand == "+" & !is.na(gene_group_right) & !is.na(exon_group_right), 
        JG_down := paste(gene_group_right, exon_group_right, sep="_")]
    junc.common[strand == "-" & !is.na(gene_group_left) & !is.na(exon_group_left), 
        JG_down := paste(gene_group_left, exon_group_left, sep="_")]

    junc.common$gene_group_left = NULL
    junc.common$gene_group_right = NULL
    junc.common$exon_group_left = NULL
    junc.common$exon_group_right = NULL
    
    irf.common.left = copy(irf.common)
    irf.common.left[, start := start - 1]
    irf.common.left[, end := start + 1]
    OL = suppressWarnings(
        findOverlaps(
            makeGRangesFromDataFrame(as.data.frame(irf.common.left)), 
            makeGRangesFromDataFrame(as.data.frame(Exon.Groups.S))
        )
    )
    irf.common[, gene_group_left := NA]
    irf.common[, exon_group_left := NA]
    irf.common$gene_group_left[OL@from] = Exon.Groups.S$gene_group[OL@to]
    irf.common$exon_group_left[OL@from] = Exon.Groups.S$exon_group[OL@to]

    irf.common.right = copy(irf.common)
    irf.common.right[, end := end + 1]
    irf.common.right[, start := end - 1]
    OL = suppressWarnings(
        findOverlaps(
            makeGRangesFromDataFrame(as.data.frame(irf.common.right)), 
            makeGRangesFromDataFrame(as.data.frame(Exon.Groups.S))
        )
    )
    irf.common[, gene_group_right := NA]
    irf.common[, exon_group_right := NA]
    irf.common$gene_group_right[OL@from] = Exon.Groups.S$gene_group[OL@to]
    irf.common$exon_group_right[OL@from] = Exon.Groups.S$exon_group[OL@to]
    
    irf.common[, JG_up := ""]
    irf.common[, JG_down := ""]
    irf.common[strand == "+" & !is.na(gene_group_left) & !is.na(exon_group_left), 
        JG_up := paste(gene_group_left, exon_group_left, sep="_")]
    irf.common[strand == "-" & !is.na(gene_group_right) & !is.na(exon_group_right), 
        JG_up := paste(gene_group_right, exon_group_right, sep="_")]
    irf.common[strand == "+" & !is.na(gene_group_right) & !is.na(exon_group_right), 
        JG_down := paste(gene_group_right, exon_group_right, sep="_")]
    irf.common[strand == "-" & !is.na(gene_group_left) & !is.na(exon_group_left), 
        JG_down := paste(gene_group_left, exon_group_left, sep="_")]

    irf.common$gene_group_left = NULL
    irf.common$gene_group_right = NULL
    irf.common$exon_group_left = NULL
    irf.common$exon_group_right = NULL
    
    if(!runStranded) {
        Exon.Groups = Exon.Groups[strand(Exon.Groups) == "*"]
    } else {
        Exon.Groups = Exon.Groups[strand(Exon.Groups) != "*"]    
    }
    irf.common.left = copy(irf.common)
    irf.common.left[, start := start - 1]
    irf.common.left[, end := start + 1]
    OL = suppressWarnings(
        findOverlaps(
            makeGRangesFromDataFrame(as.data.frame(irf.common.left)), 
            makeGRangesFromDataFrame(as.data.frame(Exon.Groups.S))
        )
    )
    irf.common[, gene_group_left := NA]
    irf.common[, exon_group_left := NA]
    irf.common$gene_group_left[OL@from] = Exon.Groups.S$gene_group[OL@to]
    irf.common$exon_group_left[OL@from] = Exon.Groups.S$exon_group[OL@to]

    irf.common.right = copy(irf.common)
    irf.common.right[, end := end + 1]
    irf.common.right[, start := end - 1]
    OL = suppressWarnings(
        findOverlaps(
            makeGRangesFromDataFrame(as.data.frame(irf.common.right)), 
            makeGRangesFromDataFrame(as.data.frame(Exon.Groups.S))
        )
    )
    irf.common[, gene_group_right := NA]
    irf.common[, exon_group_right := NA]
    irf.common$gene_group_right[OL@from] = Exon.Groups.S$gene_group[OL@to]
    irf.common$exon_group_right[OL@from] = Exon.Groups.S$exon_group[OL@to]
    
    irf.common[, IRG_up := ""]
    irf.common[, IRG_down := ""]
    irf.common[strand == "+" & !is.na(gene_group_left) & !is.na(exon_group_left), 
        IRG_up := paste(gene_group_left, exon_group_left, sep="_")]
    irf.common[strand == "-" & !is.na(gene_group_right) & !is.na(exon_group_right), 
        IRG_up := paste(gene_group_right, exon_group_right, sep="_")]
    irf.common[strand == "+" & !is.na(gene_group_right) & !is.na(exon_group_right), 
        IRG_down := paste(gene_group_right, exon_group_right, sep="_")]
    irf.common[strand == "-" & !is.na(gene_group_left) & !is.na(exon_group_left), 
        IRG_down := paste(gene_group_left, exon_group_left, sep="_")]
    irf.common$gene_group_left = NULL
    irf.common$gene_group_right = NULL
    irf.common$exon_group_left = NULL
    irf.common$exon_group_right = NULL

    irf.common[, EventRegion := paste0(seqnames, ":", start, "-", end, "/", strand)]

    Splice.Anno = as.data.table(read.fst(file.path(reference_path, "fst", "Splice.fst")))
    candidate.introns[, Event1a := Event]
    candidate.introns[, Event2a := Event]
    Splice.Anno[candidate.introns, on = "Event1a", up_1a := paste(i.gene_group_stranded, 
        i.exon_group_stranded_upstream, sep="_")]
    Splice.Anno[candidate.introns, on = "Event1a", down_1a := paste(i.gene_group_stranded, 
        i.exon_group_stranded_downstream, sep="_")]
    Splice.Anno[candidate.introns, on = "Event2a", down_2a := paste(i.gene_group_stranded, 
        i.exon_group_stranded_downstream, sep="_")]
    
    Splice.Anno[EventType %in% c("MXE", "SE", "ALE", "A3SS"),
        JG_up := up_1a]
    Splice.Anno[EventType %in% c("SE", "AFE", "A5SS"),
        JG_down := down_1a]
    Splice.Anno[EventType %in% c("MXE"),
        JG_down := down_2a]
    
    Splice.Anno$up_1a = NULL
    Splice.Anno$down_1a = NULL
    Splice.Anno$down_2a = NULL
    Splice.Anno[, strand := tstrsplit(Event1a, split="/")[[2]]]
    

	# Save irf.common, Splice.Anno
  
  if(!dir.exists(file.path(norm_output_path, "annotation"))) {
      dir.create(file.path(norm_output_path, "annotation"))
  }		  
  
	write.fst(as.data.frame(junc.common), file.path(norm_output_path, "annotation", "Junc.fst"))
	write.fst(as.data.frame(irf.common), file.path(norm_output_path, "annotation", "IR.fst"))
	write.fst(as.data.frame(Splice.Anno), file.path(norm_output_path, "annotation", "Splice.fst"))

	# make rowEvent here
	irf.anno.brief = irf.common[, c("Name", "EventRegion")]
	setnames(irf.anno.brief, "Name", "EventName")
	irf.anno.brief[, EventType := "IR"]
	irf.anno.brief = irf.anno.brief[, c("EventName", "EventType", "EventRegion")]
	
	splice.anno.brief = Splice.Anno[, c("EventName", "EventType", "EventRegion")]
	
	rowEvent = rbind(irf.anno.brief, splice.anno.brief)	
  item.todo = c("Included", "Excluded", "Depth", "Coverage", "minDepth", "Up_Inc", "Down_Inc", "Up_Exc", "Down_Exc", "junc_PSI")

  se_output_path = norm_output_path
  # if(!dir.exists(se_output_path)) {
      # dir.create(se_output_path)
  # }		

  # Annotate anything here in rowEvent.extended that allows for Annotation based filters
  tsl_min <- any_is_PC <- is_protein_coding <- all_is_NMD <- intron_id <- Inc_Is_Protein_Coding <- 
	Exc_Is_Protein_Coding <- i.intron_type <- isoform <- i.any_is_PC <- Inc_Is_NMD <- Exc_Is_NMD <- 
	splice_is_NMD <- i.splice_is_NMD <- i.IRT_is_NMD <- i.all_is_NMD <- Inc_TSL <- i.transcript_support_level <- 
	Exc_TSL <- i.tsl_min <- NULL
	
	
  # Implement filters:
    # Is_Protein_Coding: at least one of the options is a valid protein_coding transcript
    # Triggers_NMD: at least one isoform is pure NMD
    # TSL: maximum TSL of either isoform

  IR_NMD = as.data.table(read.fst(file.path(reference_path, "fst", "IR.NMD.fst")))
  # Splice_NMD = IR_NMD[, c("transcript_id", "splice_stop_pos", "splice_start_to_last_EJ", 
    # "splice_stop_to_last_EJ", "splice_is_NMD")]
  Splice.Options = as.data.table(read.fst(file.path(reference_path, "fst", "Splice.options.fst")))
  Transcripts = as.data.table(read.fst(file.path(reference_path, "fst", "Transcripts.fst")))
  # Splice.Options[Splice_NMD, on = "transcript_id", Is_NMD := i.splice_is_NMD]
  Splice.Options[Splice.Anno, on = "EventID", EventName := i.EventName]
  Splice.Options[Transcripts, on = "transcript_id", transcript_biotype := i.transcript_biotype]
    
  Splice.Options.Summary = copy(Splice.Options)
  Splice.Options.Summary[, tsl_min := min(transcript_support_level), by = c("EventID", "isoform")]
  Splice.Options.Summary[, any_is_PC := any(is_protein_coding), by = c("EventID", "isoform")]  
  Splice.Options.Summary[, all_is_NMD := all(grepl("decay", transcript_biotype)), by = c("EventID", "isoform")]  

  rowEvent.Extended = copy(rowEvent)
  
  rowEvent.Extended[EventType == "IR", intron_id := tstrsplit(EventName, split="/")[[2]]]
  rowEvent.Extended[, Inc_Is_Protein_Coding := FALSE]
  rowEvent.Extended[, Exc_Is_Protein_Coding := FALSE]
  rowEvent.Extended[IR_NMD, on = "intron_id", Exc_Is_Protein_Coding := TRUE]
  rowEvent.Extended[IR_NMD, on = "intron_id", Inc_Is_Protein_Coding := i.intron_type == "CDS"]
  
  rowEvent.Extended[Splice.Options.Summary[isoform == "A"], on = "EventName", Inc_Is_Protein_Coding := i.any_is_PC]
  rowEvent.Extended[Splice.Options.Summary[isoform == "B"], on = "EventName", Exc_Is_Protein_Coding := i.any_is_PC]
  
  rowEvent.Extended[, Inc_Is_NMD := FALSE]
  rowEvent.Extended[, Exc_Is_NMD := FALSE]
  rowEvent.Extended[IR_NMD[!is.na(splice_is_NMD)], on = "intron_id", Exc_Is_NMD := i.splice_is_NMD]
  rowEvent.Extended[IR_NMD, on = "intron_id", Inc_Is_NMD := i.IRT_is_NMD]
  rowEvent.Extended[EventType == "IR" & Exc_Is_Protein_Coding == FALSE, Exc_Is_NMD := NA]
  rowEvent.Extended[EventType == "IR" & Inc_Is_Protein_Coding == FALSE, Inc_Is_NMD := NA]

  rowEvent.Extended[Splice.Options.Summary[isoform == "A"], on = "EventName", Inc_Is_NMD := i.all_is_NMD]
  rowEvent.Extended[Splice.Options.Summary[isoform == "B"], on = "EventName", Exc_Is_NMD := i.all_is_NMD]

  rowEvent.Extended[candidate.introns, on = "intron_id", Inc_TSL := i.transcript_support_level]
  rowEvent.Extended[candidate.introns, on = "intron_id", Exc_TSL := i.transcript_support_level]

  rowEvent.Extended[Splice.Options.Summary[isoform == "A"], on = "EventName", Inc_TSL := i.tsl_min]
  rowEvent.Extended[Splice.Options.Summary[isoform == "B"], on = "EventName", Exc_TSL := i.tsl_min]

    # define Event1 / Event2
    
  rowEvent.Extended[EventType == "IR", Event1a := EventRegion]
  rowEvent.Extended[Splice.Anno, on = "EventName",
    c("Event1a", "Event2a", "Event1b", "Event2b") := 
        list(i.Event1a, i.Event2a, i.Event1b, i.Event2b)]

  write.fst(rowEvent.Extended, file.path(se_output_path, "rowEvent.fst"))

# Write junc_PSI index
  junc_PSI = junc.common[, c("seqnames", "start", "end", "strand")]
  write.fst(junc_PSI, file.path(se_output_path, "junc_PSI_index.fst"))


    rm(candidate.introns, introns.unique)
    gc()


  if(!is.null(shiny::getDefaultReactiveDomain())) {
    shiny::incProgress(0.15, message = "Generating NxtIRF FST files")
  }  
  message("Generating NxtIRF FST files")
	  
	agg.list <- suppressWarnings(BiocParallel::bplapply(seq_len(n_jobs),
		function(x, jobs, df.internal, norm_output_path) {
			suppressPackageStartupMessages({
				requireNamespace("data.table")
				requireNamespace("stats")
			})
  count <- neg <- pos <- total <- SO_L <- SO_R <- SO_I <- JG_up <- JG_down <- count_sum <- 
	count_Event1a <- Event1a <- count_Event2a <- Event2a <- count_Event1b <- Event1b <- count_Event2b <- 
	Event2b <- count_JG_up <- count_JG_down <- partic_up <- partic_down <- cov_up <- cov_down <- coverage <- 
	EventRegion <- i.EventRegion <- SpliceMax <- SpliceLeft <- SpliceRight <- SpliceOverLeft <- 
	SpliceOverRight <- SpliceOverMax <- IROratio <- IntronDepth <- TotalDepth <- Depth1a <- Depth2a <- 
	Depth1b <- Depth2b <- DepthA <- DepthB <- i.TotalDepth <- i.Depth <- ExonToIntronReadsLeft <- 
	ExonToIntronReadsRight <- NULL      
			
      # Read this from fst file
      rowEvent = as.data.table(read.fst(file.path(norm_output_path, "rowEvent.fst")))
      junc.common = as.data.table(read.fst(file.path(norm_output_path, "annotation", "Junc.fst")))
      irf.common = as.data.table(read.fst(file.path(norm_output_path, "annotation","IR.fst")))
      Splice.Anno = as.data.table(read.fst(file.path(norm_output_path, "annotation","Splice.fst")))
      
			work = jobs[[x]]
			block = df.internal[work]
			
			Included = copy(rowEvent)
			Excluded = copy(rowEvent)
			Depth = copy(rowEvent)
			Coverage = copy(rowEvent)
			minDepth = copy(rowEvent)
			
			Up_Inc = rowEvent[EventType %in% c("IR", "MXE", "SE")]
			Down_Inc = rowEvent[EventType %in% c("IR", "MXE", "SE")]
			Up_Exc = rowEvent[EventType %in% c("MXE")]		# for IR and SE, this defaults to rowEvent.Excluded
			Down_Exc = rowEvent[EventType %in% c("MXE")]		
			
            junc_PSI = as.data.table(read.fst(
                file.path(se_output_path, "junc_PSI_index.fst")
            ))
            
			for(i in seq_len(length(work))) {
				junc = as.data.table(
						read.fst(file.path(norm_output_path, "temp", paste(block$sample[i], "junc.fst.tmp", sep=".")))
				)
				junc[, start := start + 1]
				junc$strand = NULL

				junc = junc[junc.common, on = colnames(junc.common)[1:3]]
				if(block$strand[i] == 0) {
						junc$count = junc$total    
				} else if(df.internal$strand[i] == -1) {
						junc$count = 0
						junc[strand == "+", count := neg]
						junc[strand == "-", count := pos]
						junc[strand == "*", count := total]
				} else {
						junc$count = 0
						junc[strand == "+", count := pos]
						junc[strand == "-", count := neg]
						junc[strand == "*", count := total]    
				}
				junc[is.na(count), count := 0]
				junc = junc[,c("seqnames", "start", "end", "strand", "Event", "count")]
				junc = cbind(junc, junc.common[, c("JG_up", "JG_down")])
				junc[, SO_L := 0]
				junc[, SO_R := 0]
				junc[, SO_I := 0]
        
        # first overlap any junction that has non-same-island junctions
				junc[JG_up != JG_down & JG_up != "" & strand == "+", SO_L := sum(count), by = "JG_up"]
				junc[JG_up != JG_down & JG_down != "" & strand == "+", SO_R := sum(count), by = "JG_down"]
				junc[JG_up != JG_down & JG_up != "" & strand == "-", SO_R := sum(count), by = "JG_up"]
				junc[JG_up != JG_down & JG_down != "" & strand == "-", SO_L := sum(count), by = "JG_down"]
				
        # Then use a simple overlap method to account for the remainder
        junc.subset = junc[JG_up == JG_down & JG_up != "" & JG_down != ""]
        junc.gr = makeGRangesFromDataFrame(as.data.frame(junc.subset))
        OL = findOverlaps(junc.gr, junc.gr)
        # OL = OL[OL@to %in% which(junc$JG_up == junc$JG_down) | OL@from %in% which(junc$JG_up == junc$JG_down)]

        splice.overlaps.DT = data.table(from = OL@from, to = OL@to)
        splice.overlaps.DT[, count := junc.subset$count[OL@to]]
        splice.overlaps.DT[, count_sum := sum(count), by = "from"]
        splice.summa = unique(splice.overlaps.DT[, c("from", "count_sum")])        

        junc.subset[splice.summa$from, SO_I := splice.summa$count_sum]

        junc[junc.subset, on = c("Event"), SO_I := i.SO_I]
        
        junc[SO_L < SO_I, SO_L := SO_I]
        junc[SO_R < SO_I, SO_R := SO_I]
				junc[, SO_I := NULL]

				# write.fst(as.data.frame(junc), 
						# file.path(norm_output_path, "samples", paste(block$sample[i], "junc.fst", sep=".")))

				splice = copy(Splice.Anno)
				
				splice[, count_Event1a := 0]
				splice[!is.na(Event1a), count_Event1a := junc$count[match(Event1a, junc$Event)]]
				splice[is.na(count_Event1a), count_Event1a := 0]
				splice[, count_Event2a := 0]
				splice[!is.na(Event2a), count_Event2a := junc$count[match(Event2a, junc$Event)]]
				splice[is.na(count_Event2a), count_Event2a := 0]
				splice[, count_Event1b := 0]
				splice[!is.na(Event1b), count_Event1b := junc$count[match(Event1b, junc$Event)]]
				splice[is.na(count_Event1b), count_Event1b := 0]
				splice[, count_Event2b := 0]
				splice[!is.na(Event2b), count_Event2b := junc$count[match(Event2b, junc$Event)]]
				splice[is.na(count_Event2b), count_Event2b := 0]

				splice[, count_JG_up := 0]
				splice[!is.na(JG_up) & strand == "+", count_JG_up := junc$SO_L[match(JG_up, junc$JG_up)]]
				splice[!is.na(JG_up) & strand == "-", count_JG_up := junc$SO_R[match(JG_up, junc$JG_up)]]
				splice[is.na(count_JG_up), count_JG_up := 0]
				splice[, count_JG_down := 0]
				splice[!is.na(JG_down) & strand == "-", count_JG_down := junc$SO_L[match(JG_down, junc$JG_down)]]
				splice[!is.na(JG_down) & strand == "+", count_JG_down := junc$SO_R[match(JG_down, junc$JG_down)]]
				splice[is.na(count_JG_down), count_JG_down := 0]

				# Splice participation: sum of two events compared to JG_up / JG_down
				splice[, partic_up := 0]
				splice[, partic_down := 0]

				splice[EventType %in% c("MXE", "SE", "ALE", "A3SS"), partic_up := count_Event1a + count_Event1b]
				splice[EventType %in% c("MXE"), partic_down := count_Event2a + count_Event2b]
				splice[EventType %in% c("SE"), partic_down := count_Event2a + count_Event1b]
				splice[EventType %in% c("AFE", "A5SS"), partic_down := count_Event1a + count_Event1b]

				# Splice coverage = participation / max_JG
				
				splice[, cov_up := 0]
				splice[count_JG_up > 0, cov_up := partic_up / count_JG_up]
				splice[, cov_down := 0]
				splice[count_JG_down > 0, cov_down := partic_down / count_JG_down]
				splice[EventType %in% c("MXE", "SE") & cov_up < cov_down, coverage := cov_up]
				splice[EventType %in% c("MXE", "SE") & cov_up >= cov_down, coverage := cov_down]
				splice[EventType %in% c("ALE", "A3SS"), coverage := cov_up]
				splice[EventType %in% c("AFE", "A5SS"), coverage := cov_down]
				
				irf = as.data.table(
						read.fst(file.path(norm_output_path, "temp", paste(block$sample[i], "irf.fst.tmp", sep=".")))
				)
              # if(!runStranded) {
                # irf = suppressWarnings(data.table::as.data.table(
                    # data.table::fread(block$path[i], skip = "Nondir_")))
                # data.table::setnames(irf, c("Nondir_Chr", "Start", "End", "Strand"), c("seqnames","start","end", "strand"))
              # } else {
                # irf = suppressWarnings(data.table::as.data.table(
                    # data.table::fread(block$path[i], skip = "Dir_Chr")))
                # data.table::setnames(irf, c("Dir_Chr", "Start", "End", "Strand"), c("seqnames","start","end", "strand"))          
              # }
                
				irf[, start := start + 1]
				irf = irf[irf.common, on = colnames(irf.common)[1:6], EventRegion := i.EventRegion]
				
				# Extra statistics:
				irf[, SpliceMax := 0]
				irf[SpliceLeft >= SpliceRight, SpliceMax := SpliceLeft]
				irf[SpliceLeft < SpliceRight, SpliceMax := SpliceRight]

				irf[junc, on = c("seqnames", "start", "end", "strand"), SpliceOverLeft := SO_L]
				irf[junc, on = c("seqnames", "start", "end", "strand"), SpliceOverRight := SO_R]
				irf[SpliceOverLeft >= SpliceOverRight, SpliceOverMax := SpliceOverLeft]
				irf[SpliceOverLeft < SpliceOverRight, SpliceOverMax := SpliceOverRight]
				
				irf[, IROratio := 0]
				irf[IntronDepth < 1 & IntronDepth > 0 & (Coverage + SpliceOverMax) > 0, IROratio := Coverage / (Coverage + SpliceOverMax)]
				irf[IntronDepth >= 1, IROratio := IntronDepth / (IntronDepth + SpliceOverMax)]

				irf[, TotalDepth := IntronDepth + SpliceOverMax]

				splice.no_region = splice[!(EventRegion %in% irf$EventRegion)]
				splice.no_region[, Depth1a := irf$TotalDepth[match(Event1a, irf$EventRegion)]]
				splice.no_region[, Depth2a := irf$TotalDepth[match(Event2a, irf$EventRegion)]]
				splice.no_region[, Depth1b := irf$TotalDepth[match(Event1b, irf$EventRegion)]]
				splice.no_region[, Depth2b := irf$TotalDepth[match(Event2b, irf$EventRegion)]]
				splice.no_region[, Depth := 0]
				splice.no_region[count_JG_up > count_JG_down, Depth := count_JG_up]
				splice.no_region[count_JG_up <= count_JG_down, Depth := count_JG_down]
				splice.no_region[is.na(Depth1a), Depth1a := 0]
				splice.no_region[is.na(Depth1b), Depth1b := 0]
				splice.no_region[is.na(Depth2a), Depth2a := 0]
				splice.no_region[is.na(Depth2b), Depth2b := 0]
				splice.no_region[Depth1a > Depth2a, DepthA := Depth1a]
				splice.no_region[Depth1b > Depth2b, DepthB := Depth1b]
				splice.no_region[Depth1a <= Depth2a, DepthA := Depth2a]
				splice.no_region[Depth1b <= Depth2b, DepthB := Depth2b]
				splice.no_region[DepthA > DepthB, Depth := DepthA]
				splice.no_region[DepthA <= DepthB, Depth := DepthB]

				splice[, TotalDepth := 0]
				splice[irf, on = "EventRegion", TotalDepth := i.TotalDepth]
				splice[splice.no_region, on = "EventName", TotalDepth := i.Depth]

				# write.fst(as.data.frame(splice), 
						# file.path(norm_output_path, "samples", paste(block$sample[i], "splice.fst", sep=".")))
				
				# write.fst(as.data.frame(irf),
						# file.path(norm_output_path, "samples", paste(block$sample[i], "irf.fst", sep=".")))
						
				file.remove(file.path(norm_output_path, "temp", paste(block$sample[i], "junc.fst.tmp", sep=".")))
				file.remove(file.path(norm_output_path, "temp", paste(block$sample[i], "irf.fst.tmp", sep=".")))
				
				# Do BuildSE here
				setnames(irf, "Name", "EventName")
				# Included
				Included[, c(block$sample[i]) := c(
					irf$IntronDepth, 
					0.5 * (splice$count_Event1a[splice$EventType %in% c("SE", "MXE")] + 
						splice$count_Event2a[splice$EventType %in% c("SE", "MXE")]),
					splice$count_Event1a[!splice$EventType %in% c("SE", "MXE")]
				)]
				
				if(IRMode == "SpliceOverMax") {
					Excluded[, c(block$sample[i]) := c(
						irf$SpliceOverMax,
						0.5 * (splice$count_Event1b[splice$EventType %in% c("MXE")] + 
							splice$count_Event2b[splice$EventType %in% c("MXE")]),
						splice$count_Event1b[!splice$EventType %in% c("MXE")]
					)]
				} else {
					Excluded[, c(block$sample[i]) := c(
						irf$SpliceMax,
						0.5 * (splice$count_Event1b[splice$EventType %in% c("MXE")] + 
							splice$count_Event2b[splice$EventType %in% c("MXE")]),
						splice$count_Event1b[!splice$EventType %in% c("MXE")]
					)]      
				}

				# Validity checking for IR, MXE, SE
				irf[strand == "+", Up_Inc := ExonToIntronReadsLeft]
				irf[strand == "-", Up_Inc := ExonToIntronReadsRight]
				irf[strand == "+", Down_Inc := ExonToIntronReadsRight]
				irf[strand == "-", Down_Inc := ExonToIntronReadsLeft]
				
				Up_Inc[, c(block$sample[i]) := c(irf$Up_Inc, splice$count_Event1a[splice$EventType %in% c("MXE", "SE")])]
				Down_Inc[, c(block$sample[i]) := c(irf$Down_Inc, splice$count_Event2a[splice$EventType %in% c("MXE", "SE")])]
				
				Up_Exc[, c(block$sample[i]) := splice$count_Event1b[splice$EventType %in% c("MXE")]]
				Down_Exc[, c(block$sample[i]) := splice$count_Event2b[splice$EventType %in% c("MXE")]]
				
				Depth[, c(block$sample[i]) := c(irf$TotalDepth, splice$TotalDepth)]
				Coverage[, c(block$sample[i]) := c(irf$Coverage, splice$coverage)]
				
				splice[EventType %in% c("MXE", "SE") & cov_up < cov_down, minDepth := count_JG_up]
				splice[EventType %in% c("MXE", "SE") & cov_up >= cov_down, minDepth := count_JG_down]
				splice[EventType %in% c("ALE", "A3SS"), minDepth := count_JG_up]
				splice[EventType %in% c("AFE", "A5SS"), minDepth := count_JG_down]
				
				minDepth[, c(block$sample[i]) := c(
					irf$IntronDepth,
					splice$minDepth)]

                junc[count == 0, PSI := 0]
                junc[SO_L > SO_R, PSI := count / SO_L]
                junc[SO_R >= SO_L & SO_R > 0, PSI := count / SO_R]
                
				junc_PSI[junc, on = c("seqnames", "start", "end", "strand"),
                    c(block$sample[i]) := i.PSI]

                    
			} # end FOR loop
			if(low_memory_mode == TRUE) {
				# Write BuildSE temp files
				value = t(as.matrix(Included[, -c(1:3)]))
				fwrite(as.data.frame(value), file.path(norm_output_path, "temp", 
					paste("Included", as.character(x), "txt.gz", sep=".")), 
					col.names = FALSE, row.names = FALSE)
				value = t(as.matrix(Excluded[, -c(1:3)]))
				fwrite(as.data.frame(value), file.path(norm_output_path, "temp", 
					paste("Excluded", as.character(x), "txt.gz", sep=".")), 
					col.names = FALSE, row.names = FALSE)
				value = t(as.matrix(Depth[, -c(1:3)]))
				fwrite(as.data.frame(value), file.path(norm_output_path, "temp", 
					paste("Depth", as.character(x), "txt.gz", sep=".")), 
					col.names = FALSE, row.names = FALSE)
				value = t(as.matrix(Coverage[, -c(1:3)]))
				fwrite(as.data.frame(value), file.path(norm_output_path, "temp", 
					paste("Coverage", as.character(x), "txt.gz", sep=".")), 
					col.names = FALSE, row.names = FALSE)
				value = t(as.matrix(minDepth[, -c(1:3)]))
				fwrite(as.data.frame(value), file.path(norm_output_path, "temp", 
					paste("minDepth", as.character(x), "txt.gz", sep=".")), 
					col.names = FALSE, row.names = FALSE)
				value = t(as.matrix(Up_Inc[, -c(1:3)]))
				fwrite(as.data.frame(value), file.path(norm_output_path, "temp", 
					paste("Up_Inc", as.character(x), "txt.gz", sep=".")), 
					col.names = FALSE, row.names = FALSE)
				value = t(as.matrix(Down_Inc[, -c(1:3)]))
				fwrite(as.data.frame(value), file.path(norm_output_path, "temp", 
					paste("Down_Inc", as.character(x), "txt.gz", sep=".")), 
					col.names = FALSE, row.names = FALSE)
				value = t(as.matrix(Up_Exc[, -c(1:3)]))
				fwrite(as.data.frame(value), file.path(norm_output_path, "temp", 
					paste("Up_Exc", as.character(x), "txt.gz", sep=".")), 
					col.names = FALSE, row.names = FALSE)
				value = t(as.matrix(Down_Exc[, -c(1:3)]))
				fwrite(as.data.frame(value), file.path(norm_output_path, "temp", 
					paste("Down_Exc", as.character(x), "txt.gz", sep=".")), 
					col.names = FALSE, row.names = FALSE)

				value = t(as.matrix(junc_PSI[, -c(1:4)]))
				fwrite(as.data.frame(value), file.path(norm_output_path, "temp", 
					paste("junc_PSI", as.character(x), "txt.gz", sep=".")), 
					col.names = FALSE, row.names = FALSE)

				return(NULL)
			} else {
					final = list(
						Included = Included[, -c(1:3)],
						Excluded = Excluded[, -c(1:3)],
						Depth = Depth[, -c(1:3)],
						Coverage = Coverage[, -c(1:3)],
						minDepth = minDepth[, -c(1:3)],
						Up_Inc = Up_Inc[, -c(1:3)],
						Down_Inc = Down_Inc[, -c(1:3)],
						Up_Exc = Up_Exc[, -c(1:3)],
						Down_Exc = Down_Exc[, -c(1:3)],
                        junc_PSI = junc_PSI[, -c(1:4)]
					)				
				# }
			}
			
		}, df.internal = df.internal, jobs = jobs, 
			norm_output_path = norm_output_path, BPPARAM = BPPARAM_mod
	))
  gc()
	
  if(!is.null(shiny::getDefaultReactiveDomain())) {
    shiny::incProgress(0.20, message = "Building Final SummarizedExperiment Object")
  }  
  message("Building Final SummarizedExperiment Object")
  index <- NULL
	
	if(low_memory_mode) {
		for(item in item.todo) {
			file.DT = data.table(file = list.files(pattern = item, path = file.path(norm_output_path, "temp")))
			file.DT[,index := as.numeric(tstrsplit(file.DT, split=".", fixed = TRUE)[[2]])]
			setorder(file.DT, index)
			mat = NULL
			for(x in seq_len(n_jobs)) {
				temp = t(fread(file.path(file.path(norm_output_path, "temp"), file.DT$file[x]), data.table = FALSE))
				colnames(temp) = df.internal$sample[jobs[[x]]]
				mat = cbind(mat, temp)
				file.remove(file.path(file.path(norm_output_path, "temp"), file.DT$file[x]))
			}
			outfile = file.path(se_output_path, paste(item, "fst", sep="."))
			write.fst(as.data.frame(mat), outfile)
		}
  } else {
		item.DTList = list()
		for(item in item.todo) {
			for(x in seq_len(n_jobs)) {
				if(x == 1) {
					item.DTList[[item]] = agg.list[[x]][[item]]
				} else {
					item.DTList[[item]] = cbind(item.DTList[[item]], agg.list[[x]][[item]])
				}
			}
			outfile = file.path(se_output_path, paste(item, "fst", sep="."))
			write.fst(as.data.frame(item.DTList[[item]]), outfile)
		}
	}
  
	outfile = file.path(se_output_path, paste("stats", "fst", sep="."))
	write.fst(as.data.frame(df.internal), outfile)
	
    # Create barebones colData.Rds
    colData = list(
        df.files = data.table(sample = df.internal$sample),
        df.anno = data.table(sample = df.internal$sample)
    )
    saveRDS(colData, file.path(se_output_path, "colData.Rds"))
  if(!is.null(shiny::getDefaultReactiveDomain())) {
    shiny::incProgress(0.19, message = "NxtIRF Collation Finished")
  }  
  message("NxtIRF Collation Finished")
}

#' @export
MakeSE = function(fst_path, colData) {

  item.todo = c("rowEvent", "Included", "Excluded", "Depth", "Coverage", 
    "minDepth", "Up_Inc", "Down_Inc", "Up_Exc", "Down_Exc")
  files.todo = file.path(normalizePath(fst_path), paste(item.todo, "fst", sep="."))
  assert_that(all(file.exists(files.todo)),
    msg = "FST File generation appears incomplete. Suggest run CollateData() again")

  if(missing(colData)) {
      assert_that(file.exists(file.path(fst_path, "colData.Rds")),
        msg = "colData.Rds does not exist in given path")
      colData.Rds = readRds("colData.Rds")
      assert_that("df.anno" %in% names(colData.Rds),
        msg = "colData.Rds must contain df.anno containing annotations")
    
      colData = as.data.frame(colData.Rds$df.anno)
  }

  colnames(colData)[1] = "sample"
  remove_na = NULL
  if(ncol(colData) > 1) {
    for(i in seq(2, ncol(colData))) {
      if(is(colData[,i], "character")) {
        colData[,i] = factor(unlist(colData[,i]))      
      } else if(is(colData[,i], "logical")) {
        colData[,i] <- factor(unlist(ifelse(colData[,i], "TRUE","FALSE")))				
      } else if(all(is.na(unlist(colData[,i])))) {
        remove_na = append(remove_na, i)
      }
    }
  }
	if(!is.null(remove_na)) {
		colData = colData[,-remove_na]
	}
  
  rowData = read.fst(files.todo[1])
  Included = as.matrix(read.fst(files.todo[2], columns = colData$sample))
  Excluded = as.matrix(read.fst(files.todo[3], columns = colData$sample))
  Depth = as.matrix(read.fst(files.todo[4], columns = colData$sample))
  Coverage = as.matrix(read.fst(files.todo[5], columns = colData$sample))
  minDepth = as.matrix(read.fst(files.todo[6], columns = colData$sample))
  Up_Inc = as.matrix(read.fst(files.todo[7], columns = colData$sample))
  Down_Inc = as.matrix(read.fst(files.todo[8], columns = colData$sample))
  Up_Exc = as.matrix(read.fst(files.todo[9], columns = colData$sample))
  Down_Exc = as.matrix(read.fst(files.todo[10], columns = colData$sample))

  rownames(Up_Inc) = rowData$EventName[rowData$EventType %in% c("IR", "MXE", "SE")]
  rownames(Down_Inc) = rowData$EventName[rowData$EventType %in% c("IR", "MXE", "SE")]
  rownames(Up_Exc) = rowData$EventName[rowData$EventType %in% c("MXE")]
  rownames(Down_Exc) = rowData$EventName[rowData$EventType %in% c("MXE")]
  
  se = SummarizedExperiment::SummarizedExperiment(assays = S4Vectors::SimpleList(
		Included = Included, Excluded = Excluded, Depth = Depth, Coverage = Coverage, minDepth = minDepth
	),
    rowData = rowData, colData = as.data.frame(colData[, -1, drop=FALSE], row.names = colData$sample))
  rownames(se) = SummarizedExperiment::rowData(se)$EventName

  S4Vectors::metadata(se)$Up_Inc = Up_Inc
	S4Vectors::metadata(se)$Down_Inc = Down_Inc
	S4Vectors::metadata(se)$Up_Exc = Up_Exc
	S4Vectors::metadata(se)$Down_Exc = Down_Exc

  return(se)
}

#' @export
runFilter <- function(filterClass, filterType, filterVars, filterObject) {
# Internal function
  # filterClass: can be one of 'Annotation', 'Data', 'Runtime'
  # filterType:
    # - Annotation:
    # - Data:
        # - Depth: 1-minimum, 2-minCond, 3-pcTRUE
        # - Coverage: 1-minimum, 1a-minDepth, 2-minCond, 3-pcTRUE
        # - Consistency: 1-maximum, 1a-minDepth, 2-minCond, 3-pcTRUE
						# - for Consistency, maximum is the max(abs(log2_delta)) between comparison and calculated value
	
	Inc_Is_Protein_Coding <- Exc_Is_Protein_Coding <- EventType <- Inc_Is_NMD <- Exc_Is_NMD <- Inc_TSL <- 
	Exc_TSL <- NULL
	
	filterResult = rep(TRUE, nrow(filterObject))
  if(!("pcTRUE" %in% names(filterVars))) {
    usePC = 100
  } else {
    usePC = filterVars$pcTRUE
  }
  if(!("minDepth" %in% names(filterVars))) {
    minDepth = 0
  } else {
    minDepth = filterVars$minDepth
  }
	rowData = as.data.frame(SummarizedExperiment::rowData(filterObject))
	colData = as.data.frame(SummarizedExperiment::colData(filterObject))
  use_cond = ifelse(!is.null(names(filterVars)) && "condition" %in% names(filterVars) && 
    filterVars$condition %in% colnames(colData), TRUE, FALSE)
  if(filterClass == "Data") {
    if(filterType == "Depth") {
      message("Running Depth filter")
			if(!("minimum" %in% names(filterVars))) {
				minimum = 20
			} else {
				minimum = filterVars$minimum
			}
      if(use_cond == TRUE) {
        cond_vec = unlist(colData[, which(colnames(colData) == filterVars$condition)])
        cond_vars = unique(cond_vec)
      }
      depth = as.matrix(SummarizedExperiment::assay(filterObject, "Depth"))
      sum_res = rep(0, nrow(filterObject))
      if(use_cond == TRUE) {
        for(cond in cond_vars) {
          depth.subset = depth[, which(cond_vec == cond)]
          sum = rowSums(depth.subset > minimum)
          sum_res = sum_res + ifelse(sum * 100 / ncol(depth.subset) >= usePC, 1, 0)
        }
        n_TRUE = ifelse(!is.na(suppressWarnings(as.numeric(filterVars$minCond))), as.numeric(filterVars$minCond), -1)
        if(n_TRUE == -1) n_TRUE = length(cond_vars)
        res = (sum_res >= n_TRUE)
      } else {
        sum = rowSums(depth > minimum)
        res = ifelse(sum * 100 / ncol(depth) >= usePC, TRUE, FALSE)
      }
      if("EventTypes" %in% names(filterVars)) {
        res[!(rowData$EventType %in% filterVars$EventTypes)] = TRUE
      }
      return(res)
    } else if(filterType == "Coverage") {
      message("Running Coverage filter")
			if(!("minimum" %in% names(filterVars))) {
				minimum = 20
			} else {
				minimum = filterVars$minimum
			}
      if(use_cond == TRUE) {
        cond_vec = unlist(colData[, which(colnames(colData) == filterVars$condition)])
        cond_vars = unique(cond_vec)
      }
      cov = as.matrix(SummarizedExperiment::assay(filterObject, "Coverage"))
      depth = as.matrix(SummarizedExperiment::assay(filterObject, "minDepth"))
      cov[depth < minDepth] = 1    # do not test if depth below threshold
      
      sum_res = rep(0, nrow(filterObject))
      if(use_cond == TRUE) {
        for(cond in cond_vars) {
          cov.subset = cov[, which(cond_vec == cond)]
          sum = rowSums(cov.subset > minimum / 100)
          sum_res = sum_res + ifelse(sum * 100 / ncol(cov.subset) >= usePC, 1, 0)
        }
        n_TRUE = ifelse(!is.na(suppressWarnings(as.numeric(filterVars$minCond))), as.numeric(filterVars$minCond), -1)
        if(n_TRUE == -1) n_TRUE = length(cond_vars)
        res = (sum_res >= n_TRUE)
      } else {
        sum = rowSums(cov > minimum / 100)
        res = ifelse(sum * 100 / ncol(cov) >= usePC, TRUE, FALSE)
      }
      if("EventTypes" %in% names(filterVars)) {
        res[!(rowData$EventType %in% filterVars$EventTypes)] = TRUE
      }
      return(res)
    } else if(filterType == "Consistency") {	# requires: 
      message("Running Consistency filter")
			if(!("maximum" %in% names(filterVars))) {
				maximum = 1
			} else {
				maximum = filterVars$maximum
			}
      if(use_cond == TRUE) {
        cond_vec = unlist(colData[, which(colnames(colData) == filterVars$condition)])
        cond_vars = unique(cond_vec)
      }
      Up_Inc = as.matrix(S4Vectors::metadata(filterObject)$Up_Inc)
      Down_Inc = as.matrix(S4Vectors::metadata(filterObject)$Down_Inc)
			IntronDepth = as.matrix(SummarizedExperiment::assay(filterObject, "Included"))
			IntronDepth = IntronDepth[rowData$EventType %in% c("IR", "MXE", "SE"),]
      minDepth.Inc = Up_Inc + Down_Inc
      Up_Inc[minDepth.Inc < minDepth] = IntronDepth[minDepth.Inc < minDepth]    # do not test if depth below threshold
      Down_Inc[minDepth.Inc < minDepth] = IntronDepth[minDepth.Inc < minDepth]    # do not test if depth below threshold
     
			Excluded = as.matrix(SummarizedExperiment::assay(filterObject, "Excluded"))
			Excluded = Excluded[rowData$EventType %in% c("MXE"),]
      Up_Exc = as.matrix(S4Vectors::metadata(filterObject)$Up_Exc)
      Down_Exc = as.matrix(S4Vectors::metadata(filterObject)$Down_Exc)
      minDepth.Exc = Up_Exc + Down_Exc
			Up_Exc[minDepth.Exc < minDepth] = Excluded[minDepth.Exc < minDepth]    # do not test if depth below threshold
      Down_Exc[minDepth.Exc < minDepth] = Excluded[minDepth.Exc < minDepth]    # do not test if depth below threshold
			
      sum_res = rep(0, nrow(filterObject))
      if(use_cond == TRUE) {
        for(cond in cond_vars) {
          Up_Inc.subset = Up_Inc[, which(cond_vec == cond)]
          Down_Inc.subset = Down_Inc[, which(cond_vec == cond)]
					IntronDepth.subset = IntronDepth[, which(cond_vec == cond)]
          Up_Exc.subset = Up_Exc[, which(cond_vec == cond)]
          Down_Exc.subset = Down_Exc[, which(cond_vec == cond)]
					Excluded.subset = Excluded[, which(cond_vec == cond)]

					sum_inc = rowSums(
						abs(log2(Up_Inc.subset + 1) - log2(IntronDepth.subset +1)) < maximum &
						abs(log2(Down_Inc.subset + 1) - log2(IntronDepth.subset +1)) < maximum
					)
					sum_exc = rowSums(
						abs(log2(Up_Exc.subset + 1) - log2(Excluded.subset +1)) < maximum &
						abs(log2(Down_Exc.subset + 1) - log2(Excluded.subset +1)) < maximum
					)
					sum_inc = c(sum_inc, rep(ncol(Up_Inc.subset), sum(!(rowData$EventType %in% c("IR", "MXE", "SE")))))
					sum_exc = c(rep(ncol(Up_Inc.subset), sum(rowData$EventType == "IR")),
						sum_exc, rep(ncol(Up_Inc.subset), sum(!(rowData$EventType %in% c("IR", "MXE")))))
          sum = 0.5 * (sum_inc + sum_exc)
          sum_res = sum_res + ifelse(sum * 100 / ncol(Up_Inc.subset) >= usePC, 1, 0)
        }
        n_TRUE = ifelse(!is.na(suppressWarnings(as.numeric(filterVars$minCond))), as.numeric(filterVars$minCond), -1)
        if(n_TRUE == -1) n_TRUE = length(cond_vars)
        res = (sum_res >= n_TRUE)
      } else {
				sum_inc = rowSums(
					abs(log2(Up_Inc + 1) - log2(IntronDepth +1)) < filterVars$maximum &
					abs(log2(Down_Inc + 1) - log2(IntronDepth +1)) < filterVars$maximum
				)
				sum_exc = rowSums(
					abs(log2(Up_Exc + 1) - log2(Excluded +1)) < filterVars$maximum &
					abs(log2(Down_Exc + 1) - log2(Excluded +1)) < filterVars$maximum
				)
				sum_inc = c(sum_inc, rep(ncol(Up_Inc), sum(!(rowData$EventType %in% c("IR", "MXE", "SE")))))
				sum_exc = c(rep(ncol(Up_Inc), sum(rowData$EventType == "IR")),
					sum_exc, rep(ncol(Up_Inc), sum(!(rowData$EventType %in% c("IR", "MXE")))))
				sum = 0.5 * (sum_inc + sum_exc)
        res = ifelse(sum * 100 / ncol(Up_Inc) >= usePC, TRUE, FALSE)
      }
      if("EventTypes" %in% names(filterVars)) {
        res[!(SummarizedExperiment::rowData(filterObject)$EventType %in% filterVars$EventTypes)] = TRUE
      }
      return(res)

    }
  } else if(filterClass == "Annotation") {
    if(filterType == "Protein_Coding") {
      # returns if any of included or excluded is protein_coding
      rowSelected = as.data.table(rowData)
      rowSelected = rowSelected[Inc_Is_Protein_Coding == TRUE | Exc_Is_Protein_Coding == TRUE]
      rowSelected = rowSelected[EventType != "IR" | Inc_Is_Protein_Coding == TRUE] # filter for CDS introns
      res = rowData$EventName %in% rowSelected$EventName
    } else if(filterType == "NMD_Switching") {
      rowSelected = as.data.table(rowData)
      rowSelected = rowSelected[!is.na(Inc_Is_NMD) & !is.na(Exc_Is_NMD)]
      rowSelected = rowSelected[Inc_Is_NMD != Exc_Is_NMD]
      res = rowData$EventName %in% rowSelected$EventName
    } else if(filterType == "Transcript_Support_Level") {
      if(!("minimum" %in% names(filterVars))) {
				minimum = 1
			} else {
				minimum = filterVars$minimum
			}
      rowSelected = as.data.table(rowData)
      rowSelected = rowSelected[Inc_TSL != "NA" & Exc_TSL != "NA"]
      rowSelected[, Inc_TSL := as.numeric(Inc_TSL)]
      rowSelected[, Exc_TSL := as.numeric(Exc_TSL)]
      rowSelected = rowSelected[Inc_TSL <= minimum & Exc_TSL <= minimum]
      res = rowData$EventName %in% rowSelected$EventName
    }
    if("EventTypes" %in% names(filterVars)) {
      res[!(rowData$EventType %in% filterVars$EventTypes)] = TRUE
    }
    return(res)
  } else {
    return(filterResult)
  }
}

